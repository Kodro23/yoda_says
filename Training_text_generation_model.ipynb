{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORpuk/KoqLw/2sTrwxFD6V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kodro23/yoda_says/blob/main/Training_text_generation_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieve the dialogue"
      ],
      "metadata": {
        "id": "XMU7dytldzGd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "id": "PiX3gYNV_NFR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mainly followed the instructions on the unsloth github README: https://github.com/unslothai/unsloth"
      ],
      "metadata": {
        "id": "BQ7D84AtxuDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding,Trainer, TrainingArguments, AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import Dataset as HfDataset"
      ],
      "metadata": {
        "id": "gG_kqH1-CKHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train and test\n",
        "#train and test\n",
        "tokenized_dialogues = []\n",
        "yoda_dialogues = []\n",
        "for dialogue in dialogues[\"YODA\"]:\n",
        "    yoda_dialogues.append(dialogue)\n",
        "    tokens = tokenizer(dialogue, return_tensors='pt', padding=\"max_length\", truncation=True, max_length=128)\n",
        "    tokens['labels'] = tokens['input_ids'].clone()\n",
        "    tokenized_dialogues.append({k: v.squeeze(0) for k, v in tokens.items() if k in ['input_ids', 'attention_mask', 'labels']})\n",
        "\n",
        "#train test split\n",
        "train_encodings, eval_encodings, train_dialogues, eval_dialogues = train_test_split(tokenized_dialogues, yoda_dialogues, test_size=0.2, random_state=42)\n",
        "class YodaDataset(HfDataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val) for key, val in self.encodings[idx].items()}\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = YodaDataset(train_encodings)\n",
        "eval_dataset = YodaDataset(eval_encodings)\n",
        "# use the DataCollatorWithPadding to pad each batch during training.\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)"
      ],
      "metadata": {
        "id": "S8GTBTerCGo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory for model checkpoints\n",
        "    evaluation_strategy=\"steps\",     # evaluate every few steps\n",
        "    learning_rate=5e-5,              # learning rate for optimizer\n",
        "    per_device_train_batch_size=2,   # batch size for training\n",
        "    num_train_epochs=3,              # number of training epochs\n",
        "    save_steps=10_000,               # save checkpoints every 10,000 steps\n",
        "    save_total_limit=2,              # only keep the 2 most recent checkpoints\n",
        "    logging_dir='./logs',            # directory to save logs\n",
        "    logging_steps=500,               # log every 500 steps\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Set up the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the model to train\n",
        "    args=training_args,                  # training arguments\n",
        "    train_dataset=train_dataset,          # training data\n",
        "    eval_dataset=eval_dataset,             # evaluation data\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "NtV2uiRA0hDK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "outputId": "4edf8291-6039-405d-c8fd-c1157d2da140"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-37-c7a432cd1a19>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return {key: torch.tensor(val) for key, val in self.encodings[idx].items()}\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='97' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [96/96 09:02, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [96/96 09:12, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=96, training_loss=0.7531743844350179, metrics={'train_runtime': 559.7073, 'train_samples_per_second': 0.343, 'train_steps_per_second': 0.172, 'total_flos': 12542017536000.0, 'train_loss': 0.7531743844350179, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text with the fine-tuned model\n",
        "input_prompt = \"You: Am I or not?\\nYoda:\"\n",
        "inputs = tokenizer(input_prompt, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "# Ensure the attention mask is included\n",
        "input_ids = inputs['input_ids']\n",
        "attention_mask = inputs['attention_mask']\n",
        "\n",
        "# Generate text\n",
        "generated_ids = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    attention_mask=attention_mask,\n",
        "    max_length=50,           # Adjust this to a reasonable number\n",
        "    num_return_sequences=1,\n",
        "    temperature=0.7,         # Lower values make the output deterministic; higher values add randomness\n",
        "    top_p=0.9,               # Nucleus sampling for creativity\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Decode the generated text\n",
        "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "Ex3bYI3Y0hFu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47560f03-cf83-4283-dbca-1d2d1da7da6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: Am I or not?\n",
            "Yoda:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Pad Token ID:\", tokenizer.pad_token_id)\n",
        "print(\"EOS Token ID:\", tokenizer.eos_token_id)\n"
      ],
      "metadata": {
        "id": "T54reDEl0hIU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20c410a2-11bc-4acd-dafc-07ac545cee2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pad Token ID: 50256\n",
            "EOS Token ID: 50256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lmvEbmGU0hKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zBVrelmV0hNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZQOg6Vnu0hPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t0jq3NMS0hSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JLH392r40hVY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}